# Getting started with dbt and Materialize

[dbt](https://docs.getdbt.com/docs/introduction) has become the standard for data transformation (“the T in ELT”). It combines the accessibility of SQL with software engineering best practices, allowing you to not only build reliable data pipelines, but also document, test and version-control them.

While dbt is a great fit for **batch** transformations, it can only **approximate** transforming streaming data. This demo recreates the Materialize [getting started guide](https://materialize.com/docs/get-started/) using dbt as the transformation layer.

## Docker

```bash
# Start the setup
docker-compose up -d

# Check if everything is up and running!
docker-compose ps
```

Once you're done playing with the setup, tear it down:

```bash
docker-compose down -v
```

## dbt

To access the [dbt CLI](https://docs.getdbt.com/dbt-cli/cli-overview), run:

```bash
docker exec -it dbt bash
```

From here, you can run dbt commands as usual. To check that the [`dbt-materialize`](https://pypi.org/project/dbt-materialize/) plugin has been installed:

```bash
dbt --version
```

### Build and run the models

We've created a few core models that take care of defining the building blocks of a dbt+Materialize project, including a streaming [source](https://materialize.com/docs/overview/api-components/#sources):

- `market_orders_raw.sql`

, as well as a staging [view](https://materialize.com/docs/overview/api-components/#non-materialized-views) to transform the source data:

- `market_orders.sql`

and a [materialized view](https://materialize.com/docs/overview/api-components/#materialized-views) that continuously updates as the underlying data changes:

- `avg_bid.sql`

To run the models:

```bash
dbt run
```

> :crab: As an exercise, you can add models for the queries demonstrating [joins](https://materialize.com/docs/get-started/#joins) and [temporal filters](https://materialize.com/docs/get-started/#temporal-filters).

To run the tests (warnings are ok and are explained in more detail below):
```bash
dbt test
```

## Materialize

To connect to the running Materialize service, you can use `mzcli`, which is included in the setup:

```bash
docker-compose run mzcli
```

and run a few commands to check the objects created through dbt:

**Sources**

```sql
SHOW SOURCES;

       name
-------------------
 market_orders_raw
```

**Views**

```sql
SHOW VIEWS;

     name
---------------
 avg_bid
 market_orders
```

**Materialized views**

```sql
SHOW MATERIALIZED VIEWS;

  name
---------
 avg_bid
```

You'll notice that you're only able to `SELECT` from `avg_bid` — this is because it is the only materialized view! This view is incrementally updated as new data streams in, so you get fresh and correct results with low latency. Behind the scenes, Materialize is indexing the results of the embedded query in memory.

**Materialized tests**

dbt provides a framework to test assumptions about the results generated by a model, and, even better, allows you to store the results of these tests.
Because materialize is a continuously updating datasource, we can expand on dbt's existing functionality and create _materialized tests_, which will update in real time as conditions are met.

We've defined two different types of tests and configured them to be stored in two different schemas, to help demonstrate:
- `etl_failures`: ETL pipeline failures (we've included the generic `non_null` and `unique` out of the box tests). We've configured our [project](dbt/dbt_project.yml) to materialize all of our tests here.
- `bid_alerts`: Conditions specific to our business data that we might want to get alerts about. We've configured these specifically in our [model](dbt/models/marts/schema.yml) file.

```sql
SHOW SCHEMAS; 

materialize=> show schemas;
        name
--------------------
 public ---note: our data model
 public_bid_alert
 public_etl_failure
```

```sql
materialize=> show views from public_etl_failure;
          name
-------------------------
 not_null_avg_bid_symbol
 unique_avg_bid_symbol
(2 rows)
```
dbt took care of naming these for us, based on the test title and columms being tested. No rows in these! Data pipeline looks good.

```sql
materialize=> show views from public_bid_alert;
         name
-----------------------
 avg_bid_threshold_200
(1 row)
```
We gave this one an alias and set the severity to warn, since we expect that _at some point_ there could be rows that meet our condition.
An alert is firing! Does it make sense? Let's look at our avg_bid data from above:

```sql
materialize=> select * from public.avg_bid;
   symbol    |      avg_bid
-------------+--------------------
 Apple       | 201.24148328101487
 Google      | 303.28957100720896
 Elerium     | 155.55304908133022
 Bespin Gas  |  200.0016677106204
 Linen Cloth | 254.67073061687503
(5 rows)
```

And now our threshold alert:
```sql
materialize=> select * from public_bid_alert.avg_bid_threshold_200;
 symbol  |      avg_bid
---------+--------------------
 Elerium | 155.55304908133022
(1 row)
```

## Local installation

To set up dbt and Materialize in your local environment instead of using Docker, follow the instructions in the [documentation](https://materialize.com/docs/guides/dbt/).

<hr>

If you run into issues with the `dbt-materialize` adapter, please [open a GitHub issue](https://github.com/MaterializeInc/materialize/issues/new/choose) so we can look into it!
