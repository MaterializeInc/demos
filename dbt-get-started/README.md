# Getting started with dbt and Materialize

[dbt](https://docs.getdbt.com/docs/introduction) has become the standard for data transformation (“the T in ELT”). It combines the accessibility of SQL with software engineering best practices, allowing you to not only build reliable data pipelines, but also document, test and version-control them.

While dbt is a great fit for **batch** transformations, it can only **approximate** transforming streaming data. This demo recreates the Materialize [getting started guide](https://materialize.com/docs/get-started/) using dbt as the transformation layer.

## Docker

```bash
# Start the setup
docker-compose up -d

# Check if everything is up and running!
docker-compose ps
```

Once you're done playing with the setup, tear it down:

```bash
docker-compose down -v
```

## dbt

To access the [dbt CLI](https://docs.getdbt.com/dbt-cli/cli-overview), run:

```bash
docker exec -it dbt bash

#we are continously upgrading our dbt-adapter package, grab the latest!
pip install dbt-materialize --upgrade
```

From here, you can run dbt commands as usual. To check that the [`dbt-materialize`](https://pypi.org/project/dbt-materialize/) plugin has been installed:

```bash
dbt --version
```

### Build and run the models

We've created a few core models that take care of defining the building blocks of a dbt+Materialize project, including a streaming [source](https://materialize.com/docs/overview/api-components/#sources):

- `market_orders_raw.sql`

, as well as a staging [view](https://materialize.com/docs/overview/api-components/#non-materialized-views) to transform the source data:

- `market_orders.sql`

and a [materialized view](https://materialize.com/docs/overview/api-components/#materialized-views) that continuously updates as the underlying data changes:

- `avg_bid.sql`

To run the models:

```bash
dbt run
```

> :crab: As an exercise, you can add models for the queries demonstrating [joins](https://materialize.com/docs/get-started/#joins) and [temporal filters](https://materialize.com/docs/get-started/#temporal-filters).

To run the tests (warnings are ok and are explained in more detail below):

```bash
dbt test
```

## Materialize

To connect to the running Materialize service, you can use `mzcli`, which is included in the setup:

```bash
docker-compose run mzcli
```

and run a few commands to check the objects created through dbt:

**Sources**

```sql
SHOW SOURCES;

       name
-------------------
 market_orders_raw
```

**Views**

```sql
SHOW VIEWS;

     name
---------------
 alert
 avg_bid
 market_orders
```

**Materialized views**

```sql
SHOW MATERIALIZED VIEWS;

  name
---------
 avg_bid
 alert
```

You'll notice that you're only able to `SELECT` from `avg_bid` and `alert` — this is because these are the only materialized views. They are incrementally updated as new data streams in, so you get fresh and correct results with low latency. Behind the scenes, Materialize is indexing the results of the embedded query in memory.

**Materialize for Alerting**
We've included an `alert` materialized view to demonstrate how materialize can be used to know, in real time, when a condition has been met.

Peeking our avg_bid data:

```sql
SELECT * FROM public.avg_bid;

   symbol    |      avg_bid
-------------+--------------------
 Apple       | 201.24148328101487
 Google      | 303.28957100720896
 Elerium     | 155.55304908133022
 Bespin Gas  |  200.0016677106204
 Linen Cloth | 254.67073061687503
(5 rows)
```

And our alert view:

```sql
SELECT * FROM alert;

alert_condition  |       alert_labels       |    alert_value
-------------------+--------------------------+--------------------
 avg_bid_above_250 | {"symbol":"Google"}      | 304.86277762009547
 avg_bid_above_250 | {"symbol":"Linen Cloth"} | 258.85895801455746
 avg_bid_below_200 | {"symbol":"Elerium"}     | 154.27114038864772
(3 rows)
```

**Materialized tests**

dbt provides a framework to test assumptions about the results generated by a model, and, even better, allows you to store the results of these tests.
Because Materialize is a continuously updating datasource, we can expand on dbt's existing functionality and create _materialized tests_, which will update in real time.

We've included the generic `non_null` and `unique` out of the box tests in our project to help demonstrate.
We've configured our [project](dbt/dbt_project.yml) to materialize all of our tests in the `etl_failures` schema.

```sql
SHOW SCHEMAS;

        name
--------------------
 public ---note: our data model
 public_etl_failure
```

```sql
SHOW VIEWS FROM public_etl_failure;

          name
-------------------------
 not_null_avg_bid_symbol
 unique_avg_bid_symbol
(2 rows)
```

dbt took care of naming these materailized view for us based on the type of test and the columms being tested. No rows in these! Data pipeline looks good.

## Local installation

To set up dbt and Materialize in your local environment instead of using Docker, follow the instructions in the [documentation](https://materialize.com/docs/guides/dbt/).

<hr>

If you run into issues with the `dbt-materialize` adapter, please [open a GitHub issue](https://github.com/MaterializeInc/materialize/issues/new/choose) so we can look into it!
